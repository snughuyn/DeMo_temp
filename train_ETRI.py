import os
os.environ["NCCL_ASYNC_ERROR_HANDLING"] = "1"
os.environ["NCCL_DEBUG"] = "INFO"
os.environ["TORCH_DISTRIBUTED_DEBUG"] = "DETAIL"

import sys
import hydra
import pytorch_lightning as pl
from hydra.core.hydra_config import HydraConfig
from hydra.utils import instantiate
from pytorch_lightning.callbacks import (
    LearningRateMonitor,
    ModelCheckpoint,
    RichModelSummary,
    RichProgressBar,
    Callback,
    EarlyStopping
)
from pytorch_lightning.loggers import TensorBoardLogger
from pytorch_lightning.strategies import DDPStrategy
from datetime import timedelta

class SaveSelectedMetricsCallback(Callback):
    def __init__(self, filepath, patience, mode, monitor="new_minADE_avg"): # ğŸ‘ˆ ì¸ì ì¶”ê°€
        super().__init__() 
        self.filepath = filepath
        # EarlyStopping ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë‚´ë¶€ì—ì„œ ìƒì„±í•˜ì—¬ ì œì–´
        self.early_stop = EarlyStopping(
            monitor=monitor, patience=patience, mode=mode, verbose=True
        )
        # íŒŒì¼ ì²˜ìŒ ë§Œë“¤ ë•Œ í—¤ë” ì‘ì„±
        with open(self.filepath, "w") as f:
            f.write("epoch,val_minADE1,val_minADE6,val_new_minADE1,val_new_minADE6,new_minADE_avg\n")

    def on_validation_epoch_end(self, trainer, pl_module):
        metrics = trainer.callback_metrics
        epoch = trainer.current_epoch
        # ... (val_minADE1, val_minADE6, val_new_minADE1, val_new_minADE6 ì¶”ì¶œ ë° new_minADE_avg ê³„ì‚° ë¡œì§ ìœ ì§€) ...
        val_minADE1 = metrics.get("val_minADE1")
        val_minADE6 = metrics.get("val_minADE6")
        val_new_minADE1 = metrics.get("val_new_minADE1")
        val_new_minADE6 = metrics.get("val_new_minADE6")

        if val_new_minADE1 is not None and val_new_minADE6 is not None:
            new_minADE_avg = (val_new_minADE1 + val_new_minADE6) / 2
        else:
            new_minADE_avg = None

        if new_minADE_avg is not None:
            # 1. new_minADE_avgë¥¼ ê°•ì œë¡œ metricsì— ì¶”ê°€
            trainer.callback_metrics["new_minADE_avg"] = new_minADE_avg

        # 2. íŒŒì¼ ì“°ê¸° ë¡œì§ (ê¸°ì¡´ê³¼ ë™ì¼)
        if not os.path.exists(self.filepath) or os.path.getsize(self.filepath) == 0:
            with open(self.filepath, "a") as f:
                f.write("epoch,val_minADE1,val_minADE6,val_new_minADE1,val_new_minADE6,new_minADE_avg\n")

        if all(v is not None for v in [val_minADE1, val_minADE6, val_new_minADE1, val_new_minADE6]) and new_minADE_avg is not None:
            with open(self.filepath, "a") as f:
                f.write(f"{epoch},{val_minADE1:.6f},{val_minADE6:.6f},{val_new_minADE1:.6f},{val_new_minADE6:.6f},{new_minADE_avg:.6f}\n")
        
        # 3. ê³„ì‚° ì§í›„ EarlyStopping ì²´í¬ ë¡œì§ ê°•ì œ í˜¸ì¶œ
        # ì´ ì‹œì ì—ëŠ” new_minADE_avgê°€ í™•ì‹¤í•˜ê²Œ metricsì— ì¡´ì¬í•©ë‹ˆë‹¤.
        self.early_stop.on_validation_epoch_end(trainer, pl_module)

    # EarlyStoppingì˜ on_train_epoch_endë„ ê°•ì œ í˜¸ì¶œí•´ì•¼ í•©ë‹ˆë‹¤.
    def on_train_epoch_end(self, trainer, pl_module):
        self.early_stop.on_train_epoch_end(trainer, pl_module)


class CustomModelCheckpoint(ModelCheckpoint):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

    def _save_checkpoint(self, trainer, pl_module):
        metrics = trainer.callback_metrics
        # --- [ì•ˆì „ ì¥ì¹˜ ì¶”ê°€] new_minADE_avgê°€ ì—†ìœ¼ë©´ ì§ì ‘ ê³„ì‚°í•˜ì—¬ ì‚½ì… ---
        
        if "new_minADE_avg" not in metrics:
            val_new_minADE1 = metrics.get("val_new_minADE1")
            val_new_minADE6 = metrics.get("val_new_minADE6")
            
            if val_new_minADE1 is not None and val_new_minADE6 is not None:
                # í…ì„œ ë˜ëŠ” ìŠ¤ì¹¼ë¼ ê°’ ì²˜ë¦¬
                try:
                    val_new_minADE1_f = val_new_minADE1.item()
                    val_new_minADE6_f = val_new_minADE6.item()
                except AttributeError:
                    val_new_minADE1_f = val_new_minADE1
                    val_new_minADE6_f = val_new_minADE6
                    
                new_minADE_avg = (val_new_minADE1_f + val_new_minADE6_f) / 2
                
                # ê°•ì œë¡œ metricsì— ì¶”ê°€í•˜ì—¬ EarlyStoppingê³¼ Checkpointê°€ ì°¾ë„ë¡ í•¨
                metrics["new_minADE_avg"] = new_minADE_avg
        # -------------------------------------------------------------------
        
        # ê¸°ì¡´ ë¡œì§: metricsì— new_minADE_avgê°€ ìˆë‹¤ë©´ ëª¨ë‹ˆí„°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
        if "new_minADE_avg" in metrics:
            avg_metric = metrics["new_minADE_avg"]
            if avg_metric is not None:
                self.monitor = "new_minADE_avg"  # ëª¨ë‹ˆí„°ë§í•  metric ì„¤ì •
        
        # `_save_checkpoint()`ì— í•„ìš”í•œ ì¸ì ì „ë‹¬
        # ì´ì œ metricsì— new_minADE_avgê°€ í¬í•¨ë˜ì–´ ìˆê±°ë‚˜, í¬í•¨ë˜ì§€ ì•Šì•„ monitorê°€ ë³€ê²½ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë‘˜ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.
        super()._save_checkpoint(trainer, pl_module)

class SafeEarlyStopping(EarlyStopping):
    """
    EarlyStoppingì„ ìƒì†ë°›ì•„, new_minADE_avgì™€ ê°™ì€ ë™ì  ìƒì„± ì§€í‘œê°€
    ì½œë°± íƒ€ì´ë° ë¬¸ì œë¡œ ëˆ„ë½ë  ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ ê°•ì œ ê³„ì‚°/ì£¼ì… ë¡œì§ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    def on_train_epoch_end(self, trainer, pl_module):
        metrics = trainer.callback_metrics
        
        # EarlyStoppingì´ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ì§€í‘œê°€ í˜„ì¬ metricsì— ì—†ì„ ê²½ìš° ì•ˆì „ ì¥ì¹˜ ë°œë™
        if self.monitor not in metrics and self.monitor == "new_minADE_avg":
            val_new_minADE1 = metrics.get("val_new_minADE1")
            val_new_minADE6 = metrics.get("val_new_minADE6")
            
            # ê¸°ë³¸ ì§€í‘œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸ (ì§€í‘œê°€ ìˆë‹¤ë©´ í‰ê· ì„ ê³„ì‚°í•˜ì—¬ ê°•ì œ ì‚½ì…)
            if val_new_minADE1 is not None and val_new_minADE6 is not None:
                try:
                    # í…ì„œì¸ ê²½ìš° item()ìœ¼ë¡œ ìŠ¤ì¹¼ë¼ ê°’ì„ ì¶”ì¶œ
                    val_new_minADE1_f = val_new_minADE1.item()
                    val_new_minADE6_f = val_new_minADE6.item()
                except AttributeError:
                    # ì´ë¯¸ ìŠ¤ì¹¼ë¼ ê°’ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    val_new_minADE1_f = val_new_minADE1
                    val_new_minADE6_f = val_new_minADE6
                    
                new_minADE_avg = (val_new_minADE1_f + val_new_minADE6_f) / 2
                metrics["new_minADE_avg"] = new_minADE_avg
        
        # ë¶€ëª¨ í´ë˜ìŠ¤ì˜ Early Stopping ì²´í¬ ë¡œì§ ì‹¤í–‰ (ì´ì œ ì§€í‘œê°€ ì¡´ì¬í•¨)
        super().on_train_epoch_end(trainer, pl_module)

@hydra.main(version_base=None, config_path="conf", config_name="config_ETRI_lane")
def main(conf):
    pl.seed_everything(conf.seed, workers=True)
    output_dir = HydraConfig.get().runtime.output_dir

    logger = TensorBoardLogger(save_dir=output_dir, name="logs")

    # âœ… CSV ì €ì¥ Callback ì¶”ê°€
    metrics_csv_path = os.path.join(output_dir, "val_metrics.csv")
    save_metrics_callback = SaveSelectedMetricsCallback(
        filepath=metrics_csv_path,
        patience=15, 
        mode='min', 
        monitor='new_minADE_avg'
    )

    # Custom ModelCheckpoint with the adjusted monitor key
    checkpoint_callback = CustomModelCheckpoint(
        dirpath=os.path.join(output_dir, "checkpoints"),
        filename="{epoch}",
        monitor="new_minADE_avg",  # custom í‰ê·  ê°’ ëª¨ë‹ˆí„°ë§
        mode="min",
        save_top_k=10,
        save_last=True,
    )

    callbacks = [
        checkpoint_callback,
        RichModelSummary(max_depth=1),
        RichProgressBar(),
        LearningRateMonitor(logging_interval="epoch"),
        save_metrics_callback,
    ]

    trainer = pl.Trainer(
        logger=logger,
        gradient_clip_val=conf.gradient_clip_val,
        gradient_clip_algorithm=conf.gradient_clip_algorithm,
        max_epochs=conf.epochs,
        accelerator="gpu",
        devices=conf.gpus,
        strategy=DDPStrategy(
            find_unused_parameters=True,        # ìœ ì§€ (ë™ì  ê²½ë¡œ ë³´í˜¸)
            gradient_as_bucket_view=True,       # âœ… ë²„í‚· ë·°ë¡œ ë©”ëª¨ë¦¬/ë™ê¸°í™” ì•ˆì •í™”
            timeout=timedelta(minutes=30), 
            ),
        callbacks=callbacks,
        limit_train_batches=conf.limit_train_batches,
        limit_val_batches=conf.limit_val_batches,
        sync_batchnorm=conf.sync_bn,
        use_distributed_sampler=False,
    )

    model = instantiate(conf.model.target)

    os.system('cp -a %s %s' % ('conf', output_dir))
    os.system('cp -a %s %s' % ('src', output_dir))
    with open(f'{output_dir}/model.txt', 'w') as f:
        original_stdout = sys.stdout
        sys.stdout = f
        print(model)
        sys.stdout = original_stdout
        
    datamodule = instantiate(conf.datamodule.target)
    trainer.fit(model, datamodule, ckpt_path=conf.checkpoint)
    trainer.validate(model, datamodule.val_dataloader())

if __name__ == "__main__":
    main()
