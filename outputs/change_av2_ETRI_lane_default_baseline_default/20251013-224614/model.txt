Trainer(
  (net): ModelForecast(
    (hist_embed_mlp): Sequential(
      (0): Linear(in_features=4, out_features=64, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=64, out_features=128, bias=True)
    )
    (lane_query_mlp): Sequential(
      (0): Linear(in_features=256, out_features=128, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=128, out_features=128, bias=True)
    )
    (hist_embed_mamba): ModuleList(
      (0): Block(
        (mixer): Mamba(
          (in_proj): Linear(in_features=128, out_features=512, bias=False)
          (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
          (act): SiLU()
          (x_proj): Linear(in_features=256, out_features=40, bias=False)
          (dt_proj): Linear(in_features=8, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=128, bias=False)
        )
        (norm): RMSNorm()
        (drop_path): DropPath(drop_prob=0.200)
      )
      (1): Block(
        (mixer): Mamba(
          (in_proj): Linear(in_features=128, out_features=512, bias=False)
          (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
          (act): SiLU()
          (x_proj): Linear(in_features=256, out_features=40, bias=False)
          (dt_proj): Linear(in_features=8, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=128, bias=False)
        )
        (norm): RMSNorm()
        (drop_path): DropPath(drop_prob=0.200)
      )
      (2): Block(
        (mixer): Mamba(
          (in_proj): Linear(in_features=128, out_features=512, bias=False)
          (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
          (act): SiLU()
          (x_proj): Linear(in_features=256, out_features=40, bias=False)
          (dt_proj): Linear(in_features=8, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=128, bias=False)
        )
        (norm): RMSNorm()
        (drop_path): DropPath(drop_prob=0.200)
      )
      (3): Block(
        (mixer): Mamba(
          (in_proj): Linear(in_features=128, out_features=512, bias=False)
          (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
          (act): SiLU()
          (x_proj): Linear(in_features=256, out_features=40, bias=False)
          (dt_proj): Linear(in_features=8, out_features=256, bias=True)
          (out_proj): Linear(in_features=256, out_features=128, bias=False)
        )
        (norm): RMSNorm()
        (drop_path): DropPath(drop_prob=0.200)
      )
    )
    (norm_f): RMSNorm()
    (drop_path): DropPath(drop_prob=0.400)
    (lane_embed): LaneEmbeddingLayer(
      (first_conv): Sequential(
        (0): Conv1d(3, 128, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv1d(128, 256, kernel_size=(1,), stride=(1,))
      )
      (second_conv): Sequential(
        (0): Conv1d(512, 256, kernel_size=(1,), stride=(1,))
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))
      )
    )
    (pos_embed): Sequential(
      (0): Linear(in_features=4, out_features=128, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=128, out_features=128, bias=True)
    )
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (drop_path1): DropPath(drop_prob=0.200)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.2, inplace=False)
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop2): Dropout(p=0.2, inplace=False)
        )
        (drop_path2): DropPath(drop_prob=0.200)
      )
      (1): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (drop_path1): DropPath(drop_prob=0.200)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.2, inplace=False)
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop2): Dropout(p=0.2, inplace=False)
        )
        (drop_path2): DropPath(drop_prob=0.200)
      )
      (2): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (drop_path1): DropPath(drop_prob=0.200)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.2, inplace=False)
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop2): Dropout(p=0.2, inplace=False)
        )
        (drop_path2): DropPath(drop_prob=0.200)
      )
      (3): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (drop_path1): DropPath(drop_prob=0.200)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.2, inplace=False)
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop2): Dropout(p=0.2, inplace=False)
        )
        (drop_path2): DropPath(drop_prob=0.200)
      )
      (4): Block(
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
        )
        (drop_path1): DropPath(drop_prob=0.200)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (act): GELU(approximate='none')
          (drop1): Dropout(p=0.2, inplace=False)
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop2): Dropout(p=0.2, inplace=False)
        )
        (drop_path2): DropPath(drop_prob=0.200)
      )
    )
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    (dense_predictor): Sequential(
      (0): Linear(in_features=128, out_features=256, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=256, out_features=120, bias=True)
    )
    (time_embedding_mlp): Sequential(
      (0): Linear(in_features=1, out_features=64, bias=True)
      (1): GELU(approximate='none')
      (2): Linear(in_features=64, out_features=128, bias=True)
    )
    (time_decoder): TimeDecoder(
      (cross_block_time): ModuleList(
        (0): Cross_Block(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (drop_path1): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.2, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.2, inplace=False)
          )
          (drop_path2): DropPath(drop_prob=0.200)
        )
        (1): Cross_Block(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (drop_path1): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.2, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.2, inplace=False)
          )
          (drop_path2): DropPath(drop_prob=0.200)
        )
      )
      (timequery_embed_mamba): ModuleList(
        (0): Block(
          (mixer): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (conv1d_b): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (x_proj_b): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj_b): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
          (norm): RMSNorm()
          (drop_path): DropPath(drop_prob=0.200)
        )
        (1): Block(
          (mixer): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (conv1d_b): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (x_proj_b): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj_b): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
          (norm): RMSNorm()
          (drop_path): DropPath(drop_prob=0.200)
        )
      )
      (timequery_norm_f): RMSNorm()
      (timequery_drop_path): DropPath(drop_prob=0.200)
      (dense_predict): Sequential(
        (0): Linear(in_features=128, out_features=256, bias=True)
        (1): GELU(approximate='none')
        (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (3): Linear(in_features=256, out_features=128, bias=True)
        (4): GELU(approximate='none')
        (5): Linear(in_features=128, out_features=64, bias=True)
        (6): GELU(approximate='none')
        (7): Linear(in_features=64, out_features=2, bias=True)
      )
      (future_embedding): Sequential(
        (0): Linear(in_features=420, out_features=128, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=128, out_features=128, bias=True)
      )
      (future_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
      )
      (self_block_mode): ModuleList(
        (0): Block(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (drop_path1): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.2, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.2, inplace=False)
          )
          (drop_path2): DropPath(drop_prob=0.200)
        )
        (1): Block(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (drop_path1): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.2, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.2, inplace=False)
          )
          (drop_path2): DropPath(drop_prob=0.200)
        )
        (2): Block(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (drop_path1): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.2, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.2, inplace=False)
          )
          (drop_path2): DropPath(drop_prob=0.200)
        )
      )
      (cross_block_mode): ModuleList(
        (0): Cross_Block(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (drop_path1): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.2, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.2, inplace=False)
          )
          (drop_path2): DropPath(drop_prob=0.200)
        )
        (1): Cross_Block(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (drop_path1): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.2, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.2, inplace=False)
          )
          (drop_path2): DropPath(drop_prob=0.200)
        )
        (2): Cross_Block(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (drop_path1): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.2, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.2, inplace=False)
          )
          (drop_path2): DropPath(drop_prob=0.200)
        )
      )
      (multi_modal_query_embedding): Embedding(6, 128)
      (predictor): GMMPredictor(
        (gaussian): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=256, out_features=120, bias=True)
        )
        (score): Sequential(
          (0): Linear(in_features=128, out_features=64, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=64, out_features=1, bias=True)
        )
        (scale): Sequential(
          (0): Linear(in_features=128, out_features=256, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=256, out_features=120, bias=True)
        )
      )
      (self_block_dense): ModuleList(
        (0): Block(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (drop_path1): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.2, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.2, inplace=False)
          )
          (drop_path2): DropPath(drop_prob=0.200)
        )
        (1): Block(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (drop_path1): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.2, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.2, inplace=False)
          )
          (drop_path2): DropPath(drop_prob=0.200)
        )
        (2): Block(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (drop_path1): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.2, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.2, inplace=False)
          )
          (drop_path2): DropPath(drop_prob=0.200)
        )
      )
      (cross_block_dense): ModuleList(
        (0): Cross_Block(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (drop_path1): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.2, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.2, inplace=False)
          )
          (drop_path2): DropPath(drop_prob=0.200)
        )
        (1): Cross_Block(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (drop_path1): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.2, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.2, inplace=False)
          )
          (drop_path2): DropPath(drop_prob=0.200)
        )
        (2): Cross_Block(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (drop_path1): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.2, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.2, inplace=False)
          )
          (drop_path2): DropPath(drop_prob=0.200)
        )
      )
      (self_block_different_mode): ModuleList(
        (0): Block(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (drop_path1): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.2, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.2, inplace=False)
          )
          (drop_path2): DropPath(drop_prob=0.200)
        )
        (1): Block(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (drop_path1): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.2, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.2, inplace=False)
          )
          (drop_path2): DropPath(drop_prob=0.200)
        )
        (2): Block(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)
          )
          (drop_path1): DropPath(drop_prob=0.200)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.2, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.2, inplace=False)
          )
          (drop_path2): DropPath(drop_prob=0.200)
        )
      )
      (dense_embed_mamba): ModuleList(
        (0): Block(
          (mixer): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (conv1d_b): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (x_proj_b): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj_b): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
          (norm): RMSNorm()
          (drop_path): DropPath(drop_prob=0.200)
        )
        (1): Block(
          (mixer): Mamba(
            (in_proj): Linear(in_features=128, out_features=512, bias=False)
            (conv1d): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (act): SiLU()
            (x_proj): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj): Linear(in_features=8, out_features=256, bias=True)
            (conv1d_b): Conv1d(256, 256, kernel_size=(4,), stride=(1,), padding=(3,), groups=256)
            (x_proj_b): Linear(in_features=256, out_features=40, bias=False)
            (dt_proj_b): Linear(in_features=8, out_features=256, bias=True)
            (out_proj): Linear(in_features=256, out_features=128, bias=False)
          )
          (norm): RMSNorm()
          (drop_path): DropPath(drop_prob=0.200)
        )
      )
      (dense_norm_f): RMSNorm()
      (dense_drop_path): DropPath(drop_prob=0.200)
      (predictor_dense): GMMPredictor_dense(
        (gaussian): Sequential(
          (0): Linear(in_features=128, out_features=64, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=64, out_features=2, bias=True)
        )
        (score): Sequential(
          (0): Linear(in_features=128, out_features=64, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=64, out_features=1, bias=True)
        )
        (scale): Sequential(
          (0): Linear(in_features=128, out_features=64, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=64, out_features=2, bias=True)
        )
      )
      (obj_pos_encoding_layer): Sequential(
        (0): Linear(in_features=2, out_features=128, bias=True)
        (1): ReLU()
        (2): Linear(in_features=128, out_features=128, bias=True)
        (3): ReLU()
        (4): Linear(in_features=128, out_features=128, bias=True)
      )
      (dense_future_head): Sequential(
        (0): Linear(in_features=256, out_features=128, bias=False)
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU()
        (3): Linear(in_features=128, out_features=128, bias=False)
        (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU()
        (6): Linear(in_features=128, out_features=420, bias=True)
      )
      (future_traj_mlps): Sequential(
        (0): Linear(in_features=240, out_features=128, bias=True)
        (1): ReLU()
        (2): Linear(in_features=128, out_features=128, bias=True)
        (3): ReLU()
        (4): Linear(in_features=128, out_features=128, bias=True)
      )
      (traj_fusion_mlps): Sequential(
        (0): Linear(in_features=256, out_features=128, bias=True)
        (1): ReLU()
        (2): Linear(in_features=128, out_features=128, bias=True)
        (3): ReLU()
        (4): Linear(in_features=128, out_features=128, bias=True)
      )
    )
  )
  (val_metrics): MetricCollection(
    (MR): MR()
    (b-minFDE6): brier_minFDE()
    (minADE1): minADE()
    (minADE6): minADE()
    (minFDE1): minFDE()
    (minFDE6): minFDE(),
    prefix=val_
  )
  (val_metrics_new): MetricCollection(
    (MR): MR()
    (b-minFDE6): brier_minFDE()
    (minADE1): minADE()
    (minADE6): minADE()
    (minFDE1): minFDE()
    (minFDE6): minFDE(),
    prefix=val_new_
  )
)
